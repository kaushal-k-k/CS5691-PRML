# -*- coding: utf-8 -*-
"""PRML_assignment2_question2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TF6zhgmHDJWedziwOfBDe5usd4sGG4qj
"""

from google.colab import drive
drive.mount('/content/drive')

import glob
import numpy as np
import math
import pandas as pd
import random
import scipy.linalg as la
import matplotlib.pyplot as plt
import cv2

def load_data(path_train,path_test):
  data=pd.read_csv(path_train,header=None)
  X_train=data.iloc[:,:-1]
  Y_train=data.iloc[:,-1]
  test = pd.read_csv(path_test,header=None)
  X_test=test.iloc[:,:-1]
  Y_test=test.iloc[:,-1]

  return X_train,Y_train,X_test,Y_test

X_train,Y_train,X_test,Y_test=load_data('drive/My Drive/prml assignment/assignment 2/question 2/A2Q2Data_test.csv','drive/My Drive/prml assignment/assignment 2/question 2/A2Q2Data_train.csv')
X_train = np.matrix(X_train)
Y_train = np.matrix(Y_train).T
X_test = np.matrix(X_test)
Y_test = np.matrix(Y_test).T
# print(Y_train.shape)

"""# linear regression analytical solution

"""

def linear_regression():
  T = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ Y_train
  return T

W_ml = linear_regression() 
print(W_ml)

"""# gradient descent"""

def gradient_descent(iterations,n=0.000001):
  W= np.matrix(np.ones((100,1)))
  # print(W)
  C = X_train.T @ X_train
  D = X_train.T @ Y_train
  W_ml = np.matrix(linear_regression())
  # print(W.T.shape,W_ml.T.shape)
  error = []
  for i in range(iterations):
    W = W - n * (C @ W - D)
    # print(C )
    # print( np.power(np.sum(np.power((np.subtract(W , W_ml)),2)),1/2))
    error.append( np.power(np.sum(np.power((np.subtract(W , W_ml)),2)),1/2) )

  return W,error

W_gd , error = gradient_descent(6000)
# print(error)
# W= np.matrix(np.ones((100,1)))
# print(np.power(np.sum(np.power((np.subtract(W - 0.01 * (X_train.T @ X_train @ W - X_train.T @ Y_train) , 0.1* W_ml)),2)),1/2))

plt.plot(error)
plt.xlabel("x-axis/no of iterations (t)")
plt.ylabel("y-axis/ least square error (|w_t - w_ml|)")
plt.savefig("gradient.png",dpi=300)
# print(W_ml)
# print(W_ml[0:10] , W_gd[0:10])
# print(error)

"""# stochastic gradient descent"""

def stochastic_graient_descent(iterations,batch_size=100,n=0.0001):
  no_of_batch = int(len(X_train)/batch_size)
  # print(no_of_batch,iterations/no_of_batch)
  iterations = int(iterations/no_of_batch)
  # print(iterations)
  errors=[]
  W = np.matrix(np.ones((100,1)))
  W_ml = np.matrix(linear_regression())
  batch_X = [ X_train[(i*batch_size):(i+1)*batch_size] for i in range(no_of_batch)]
  batch_Y = [Y_train[(i*batch_size):(i+1)*batch_size] for i in range(no_of_batch)]

  for i in range(iterations):
    for j in range(no_of_batch):
      X = np.matrix(batch_X[j])
      # print(X.shape)
      Y = np.matrix(batch_Y[j])
      W = W - n*( X.T @ X @ W - X.T @ Y)
      errors.append( np.power(np.sum(np.power((np.subtract(W , W_ml)),2)),1/2) )

  return W,errors

W_sgd,error_sgd = stochastic_graient_descent(5000)

plt.plot(error_sgd)
plt.xlabel("x-axis/no of batches (t) (batch size=100)")
plt.ylabel("y-axis/ least square error (|w_t - w_ml|)")
plt.savefig("stochastic.png",dpi=300)
# print(error_sgd)

plt.plot(error_sgd,"r")
plt.plot(error,"b")

"""# ridge regression"""

def ridge_regression(iterations,l_reg,n=0.000001):
  W= np.matrix(np.ones((100,1)))
  # print(W)
  #  70 - 30 split of train for cross validation
  px_train = X_train[:9700][:]
  py_train = Y_train[:9700]
  # print(px_train.shape,py_train.shape)
  px_test = X_train[9700:] [:]
  py_test = Y_train[9700:]
  # print(px_test.shape,py_test.shape)
  C = px_train.T @ px_train
  D = px_train.T @ py_train

  W_ml = np.matrix(linear_regression())
  # print(W.T.shape,W_ml.T.shape)
  
  for i in range(iterations):
    W = W - n* (C @ W - D + l_reg * np.identity(100) @ W)
    # print(C )
    # print( np.power(np.sum(np.power((np.subtract(W , W_ml)),2)),1/2))
   
  errorr = np.sum(np.power((np.subtract(py_test , px_test @ W)),2))/len(py_test)
  return W,errorr

lambda_reg=[1 * i/10 for i in range(40,47)]
# for i in range(0,20):
#   lambda_reg.append(i*10)
# lambda_reg.sort()
error_l=[]
W_temp = []
for i in lambda_reg:
  t,e=ridge_regression(6000,i)
  error_l.append(e)
  W_temp.append(t)

# print(error_l)

print(lambda_reg)
plt.plot(lambda_reg,error_l)
plt.xlabel("x-axis/lambda")
plt.ylabel("y-axis/ MSE for validation set (30% of train set)")
plt.savefig("reg.png",dpi=300)
# print(Y_train.shape)

test_error_ml = np.sum(np.power((np.subtract(Y_test , X_test @ W_ml)),2))/len(Y_test)
#  for -150 = lambda 
W_t_reg  = W_temp[lambda_reg.index(4.2)]
# print(lambda_reg.index(4.3))
test_error_reg = np.sum(np.power((np.subtract(Y_test , X_test @ W_t_reg)),2))/len(Y_test)
# test_error_reg = error_l[4]

print(test_error_ml , test_error_reg)
print("percentage difference in error",(test_error_ml-test_error_reg)*100)